\documentclass[10pt,pdf,hyperref={unicode}]{beamer}

\mode<presentation>
{
\usetheme{boxes}
\beamertemplatenavigationsymbolsempty

\setbeamertemplate{footline}[page number]
\setbeamersize{text margin left=0.5em, text margin right=0.5em}
}

\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{bm}
\usepackage{multirow}
\usepackage{ragged2e}
\usepackage{indentfirst}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\usepackage{mathtools}
\usepackage{comment}
\usepackage{multicol}

\usepackage[all]{xy}

\usepackage{tikz}
\usetikzlibrary{positioning,arrows}

\tikzstyle{name} = [parameters]
\definecolor{name}{rgb}{0.5,0.5,0.5}

\usepackage{caption}
\captionsetup{skip=0pt,belowskip=0pt}

% colors
\definecolor{darkgreen}{rgb}{0.0, 0.2, 0.13}
\definecolor{darkcyan}{rgb}{0.0, 0.55, 0.55}

\AtBeginEnvironment{figure}{\setcounter{subfigure}{0}}

\captionsetup[subfloat]{labelformat=empty}
\begin{document}


%---------------------------------------------------------------------------------------------------------
\section{Ensemble}
\begin{frame}{Theoretical background}
    The proposed problem is the problem of stochastic matrix factorization. For it to have a unique solution, it has to be regularized as follows:
\begin{equation}
    \sum_{d, w} n_{dw} \ln \sum_{t} \phi_{wt}\theta_{td} + R(\Phi, \Theta) \rightarrow \max_{\Phi, \Theta}
    \label{likelihood}
\end{equation}
Where $R(\Phi, \Theta$) --- additive regularization term. This is the classic problem of ARTM. BigARTM authors proposed a solution using expectation-maximization algorithm:
\begin{equation}
    \begin{cases}
        p_{tdw} \equiv p(t|d, w) = \underset{t \in T}{norm} \\
        \phi_{wt} = \underset{w \in W}{norm}\left(n_{wt} + \phi_{wt}\frac{\partial R}{\partial \phi_{wt}}\right), \quad n_{wt} = \underset{d \in D}{\sum} n_{dw}p_{tdw} \\
        \theta_{td} = \underset{t \in T}{norm}\left(n_{td} + \theta_{td}\frac{\partial R}{\partial \theta_{td}}\right), \quad n_{td} = \underset{w \in d}{\sum} n_{dw}p_{tdw} \\
    \end{cases}
    \label{em}
\end{equation}
\end{frame}

\begin{frame}{Quick comparison to sota-model}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\linewidth]{Screenshot 2024-12-20 at 15.51.52.png}
        \label{fig:enter-label}
    \end{figure}
\end{frame}

\begin{frame}{Model architecture}
    \begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Screenshot 2024-10-11 at 12.42.18.png}
    \label{fig:enter-label}
\end{figure}
\end{frame}

\begin{frame}{Experiments}

We evaluate the proposed model on the 20newsgroups dataset, which contains a collection of approximately 20,000 newsgroup documents, partitioned nearly evenly across 20 different newsgroups. We extracted approximately 70,000 unique words as vocabulary in bag-of-words format.

    \begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Screenshot 2024-11-22 at 18.04.36.png}
    \caption{Quantiative results}
    \label{fig:enter-label1}
\end{figure}
\end{frame}

\begin{frame}{Qualitative Experiments}
    We analyze our outcomes in terms of top-k words, representing the topic, focusing on models with 10 topics. These particular examples demonstrate the same logic of clusterization in both models.\\

\begin{multicols}{2}
\section*{BigARTM, top-5 topic words}
topic 1: max, q, r, g, p\\
topic 2: one, would, say, people, write\\
topic 3: game, team, line, subject, organization\\
topic 4: would, people, write, gun, article\\
topic 5: god, hell, atheist, line, subject\\
topic 6: x, file, use, window, program\\
topic 7: say, armenian, people, one, go\\
topic 8: line, subject, get, organization, car\\
topic 9: space, organization, subject, line, db\\
topic 10: line, subject, organization, use, university\\

\columnbreak

\section*{Ours, top-5 topic words}
topic 1: max, q, r, g, p\\
topic 2: line, one, get, subject, use\\
topic 3: god, would, say, one, people\\
topic 4: line, subject, organization, university, article\\
topic 5: game, team, line, drive, subject\\
topic 6: say, armenian, one, go, people\\
topic 7: x, file, line, use, subject\\
topic 8: use, line, subject, organization, window\\
topic 9: would, people, write, get, article\\
topic 10: use, key, system, data, space\\
\end{multicols}
\end{frame}


\end{document} 